{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Cell Fingerprinting via Network Traffic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following functions compute a statistic from a pcap dump, that might be used as a feature for the classifer\n",
    "\"\"\"\n",
    "\n",
    "# Compute the number of incoming packets\n",
    "def incoming_packets(dump_df, src_ip):\n",
    "    return dump_df[dump_df[\"ip.dst\"] == src_ip].count()[0]\n",
    "\n",
    "# Compute the number of outgoing packets\n",
    "def outgoing_packets(dump_df, src_ip):\n",
    "    return dump_df[dump_df[\"ip.src\"] == src_ip].count()[0]\n",
    "\n",
    "# Compute the total number of packets\n",
    "def total_number_of_packets(dump_df):\n",
    "    return dump_df.count()[0]\n",
    "\n",
    "# Ratio of incoming packets vs. outgoing packets\n",
    "def in_out_ratio(dump_df, src_ip):\n",
    "    in_count = float(incoming_packets(dump_df, src_ip))\n",
    "    out_count = outgoing_packets(dump_df, src_ip)\n",
    "    return in_count / (out_count if out_count else 1)\n",
    "\n",
    "# Total size of incoming packets\n",
    "def incoming_size(dump_df, src_ip):\n",
    "    return dump_df[dump_df[\"ip.dst\"] == src_ip][\"frame.len\"].sum()\n",
    "\n",
    "# Total size of outgoing packets\n",
    "def outgoing_size(dump_df, src_ip):\n",
    "    return dump_df[dump_df[\"ip.src\"] == src_ip][\"frame.len\"].sum()\n",
    "\n",
    "# Total size of packets\n",
    "def total_size(dump_df):\n",
    "    return dump_df[\"frame.len\"].sum()\n",
    "\n",
    "# Ratio of total incoming packet size vs total outgoing packet size\n",
    "def size_ratio(dump_df, src_ip):\n",
    "    in_size = float(incoming_size(dump_df, src_ip))\n",
    "    out_size = outgoing_size(dump_df, src_ip)\n",
    "    return in_size / (out_size if out_size else 1)\n",
    "\n",
    "# Relative time of last packet\n",
    "def relative_end_time(dump_df):\n",
    "    return float(dump_df.iloc[-1][\"frame.time_relative\"])\n",
    "\n",
    "# Mean and standard deviation of outgoing packet orderings\n",
    "def ordering_statistics(dump_df, src_ip):\n",
    "    outgoing_frame_numbers = dump_df[dump_df[\"ip.src\"] == src_ip][\"frame.number\"].values\n",
    "    return {\"OutgoingOrderingMean\": outgoing_frame_numbers.mean(), \"OutgoingOrderingSTD\": outgoing_frame_numbers.std()}\n",
    "\n",
    "# Percentages of incoming and outgoing packets in a given time interval\n",
    "def timing_info(dump_df, src_ip, n_intervals):\n",
    "    end_time = float(dump_df.iloc[-1][\"frame.time_relative\"])\n",
    "    time_intervals = np.linspace(0, end_time, n_intervals+1, endpoint=True)[1:]\n",
    "    timing_info = dump_df[\"frame.time_relative\"]\\\n",
    "                    .apply(lambda ts: next(idx for idx, interval_limit in enumerate(time_intervals) if ts <= interval_limit))\n",
    "    dump_df[\"frame.timing_interval\"] = timing_info\n",
    "    in_stream = []\n",
    "    out_stream = []\n",
    "    in_size = 0\n",
    "    out_size = 0\n",
    "    for i in range(n_intervals):\n",
    "        frames_in_interval = dump_df[dump_df[\"frame.timing_interval\"] == i]\n",
    "        in_size += frames_in_interval[frames_in_interval[\"ip.dst\"] == src_ip][\"frame.len\"].sum()\n",
    "        out_size += frames_in_interval[frames_in_interval[\"ip.src\"] == src_ip][\"frame.len\"].sum()\n",
    "        in_stream.append(float(in_size))\n",
    "        out_stream.append(float(out_size))\n",
    "    in_stream_features = {f\"InStream{i+1}\": 100 * (s / in_size) for i, s in enumerate(in_stream)}\n",
    "    out_stream_features = {f\"OutStream{i+1}\": 100 * (s / out_size) for i, s in enumerate(out_stream)}\n",
    "    return dict(in_stream_features, **out_stream_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for a pcap dump, that will be used as features\n",
    "def get_features(dump_df):\n",
    "    src_ip = dump_df.iloc[0][\"ip.src\"]\n",
    "    features = {}\n",
    "    \n",
    "    features[\"Duration\"] = relative_end_time(dump_df)\n",
    "    features[\"TotalPackets\"] = total_number_of_packets(dump_df)\n",
    "    features[\"IncomingPackets\"] = incoming_packets(dump_df, src_ip)\n",
    "    features[\"OutgoingPackets\"] = outgoing_packets(dump_df, src_ip)\n",
    "    features[\"InOutRatio\"] = in_out_ratio(dump_df, src_ip)\n",
    "    features[\"TotalSize\"] = total_size(dump_df)\n",
    "    features[\"IncomingSize\"] = incoming_size(dump_df, src_ip)\n",
    "    features[\"OutgoingSize\"] = outgoing_size(dump_df, src_ip)\n",
    "    features[\"SizeRatio\"] = size_ratio(dump_df, src_ip)\n",
    "    features.update(ordering_statistics(dump_df, src_ip))\n",
    "    features.update(timing_info(dump_df, src_ip, 20))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dump_folder = \"csv\"\n",
    "\n",
    "\"\"\"\n",
    "cell_dumps = [[os.path.join(csv_dump_folder, f\"cellID{cell_id+1}\", f) for f in os.listdir(os.path.join(csv_dump_folder, f\"cellID{cell_id+1}\"))\\\n",
    "                        if f[:4] == \"dump\"]\\\n",
    "                for cell_id in range(100)]\n",
    "\n",
    "min_dumps = min([len(dump_files) for dump_files in cell_dumps])\n",
    "cell_dumps = [dump_files[:min_dumps] for dump_files in cell_dumps]\n",
    "\"\"\"\n",
    "\n",
    "#cell_dumps = [[os.path.join(csv_dump_folder, f\"cellID{cell_id}\", f\"dump{i}.csv\") for i in range(1,101)] for cell_id in range(1,11)]\n",
    "cell_dumps = [[os.path.join(csv_dump_folder, f\"cellID{cell_id}\", f\"dump{i}.csv\") for i in range(1,21)] for cell_id in range(1,101)]\n",
    "\n",
    "feature_set = []\n",
    "labels = []\n",
    "for cell_id, dumps in enumerate(cell_dumps):\n",
    "    for dump_file in dumps:\n",
    "        dump_df = pd.read_csv(dump_file, names=[\"frame.number\", \"frame.time_relative\", \"ip.src\" ,\"ip.dst\", \"frame.protocols\", \"frame.len\"])\n",
    "        if dump_df.count()[0]:\n",
    "            features = get_features(dump_df)\n",
    "            feature_set.append(features)\n",
    "            labels.append(cell_id)\n",
    "            \n",
    "X = pd.DataFrame(feature_set)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Duration</th>\n",
       "      <th>TotalPackets</th>\n",
       "      <th>IncomingPackets</th>\n",
       "      <th>OutgoingPackets</th>\n",
       "      <th>InOutRatio</th>\n",
       "      <th>TotalSize</th>\n",
       "      <th>IncomingSize</th>\n",
       "      <th>OutgoingSize</th>\n",
       "      <th>SizeRatio</th>\n",
       "      <th>OutgoingOrderingMean</th>\n",
       "      <th>...</th>\n",
       "      <th>OutStream11</th>\n",
       "      <th>OutStream12</th>\n",
       "      <th>OutStream13</th>\n",
       "      <th>OutStream14</th>\n",
       "      <th>OutStream15</th>\n",
       "      <th>OutStream16</th>\n",
       "      <th>OutStream17</th>\n",
       "      <th>OutStream18</th>\n",
       "      <th>OutStream19</th>\n",
       "      <th>OutStream20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.051183</td>\n",
       "      <td>221</td>\n",
       "      <td>175</td>\n",
       "      <td>46</td>\n",
       "      <td>3.804348</td>\n",
       "      <td>378195</td>\n",
       "      <td>348485</td>\n",
       "      <td>29710</td>\n",
       "      <td>11.729552</td>\n",
       "      <td>239.217391</td>\n",
       "      <td>...</td>\n",
       "      <td>58.296870</td>\n",
       "      <td>66.240323</td>\n",
       "      <td>68.226186</td>\n",
       "      <td>70.212050</td>\n",
       "      <td>72.197913</td>\n",
       "      <td>74.183777</td>\n",
       "      <td>80.141367</td>\n",
       "      <td>92.056547</td>\n",
       "      <td>94.042410</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.994780</td>\n",
       "      <td>174</td>\n",
       "      <td>136</td>\n",
       "      <td>38</td>\n",
       "      <td>3.578947</td>\n",
       "      <td>349012</td>\n",
       "      <td>324536</td>\n",
       "      <td>24476</td>\n",
       "      <td>13.259356</td>\n",
       "      <td>191.631579</td>\n",
       "      <td>...</td>\n",
       "      <td>61.431606</td>\n",
       "      <td>63.842131</td>\n",
       "      <td>75.894754</td>\n",
       "      <td>75.894754</td>\n",
       "      <td>78.305279</td>\n",
       "      <td>80.715803</td>\n",
       "      <td>83.126328</td>\n",
       "      <td>85.536852</td>\n",
       "      <td>87.947377</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.128145</td>\n",
       "      <td>250</td>\n",
       "      <td>202</td>\n",
       "      <td>48</td>\n",
       "      <td>4.208333</td>\n",
       "      <td>425902</td>\n",
       "      <td>396040</td>\n",
       "      <td>29862</td>\n",
       "      <td>13.262340</td>\n",
       "      <td>257.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>52.581877</td>\n",
       "      <td>56.533387</td>\n",
       "      <td>64.436407</td>\n",
       "      <td>70.363673</td>\n",
       "      <td>72.339428</td>\n",
       "      <td>76.290938</td>\n",
       "      <td>84.193959</td>\n",
       "      <td>86.169714</td>\n",
       "      <td>94.072735</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.010340</td>\n",
       "      <td>208</td>\n",
       "      <td>166</td>\n",
       "      <td>42</td>\n",
       "      <td>3.952381</td>\n",
       "      <td>381302</td>\n",
       "      <td>354466</td>\n",
       "      <td>26836</td>\n",
       "      <td>13.208600</td>\n",
       "      <td>225.357143</td>\n",
       "      <td>...</td>\n",
       "      <td>53.830675</td>\n",
       "      <td>60.426293</td>\n",
       "      <td>69.220450</td>\n",
       "      <td>71.418989</td>\n",
       "      <td>75.816068</td>\n",
       "      <td>78.014607</td>\n",
       "      <td>80.213147</td>\n",
       "      <td>82.411686</td>\n",
       "      <td>91.205843</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.010757</td>\n",
       "      <td>214</td>\n",
       "      <td>175</td>\n",
       "      <td>39</td>\n",
       "      <td>4.487179</td>\n",
       "      <td>357960</td>\n",
       "      <td>331866</td>\n",
       "      <td>26094</td>\n",
       "      <td>12.718096</td>\n",
       "      <td>238.717949</td>\n",
       "      <td>...</td>\n",
       "      <td>50.548019</td>\n",
       "      <td>57.331187</td>\n",
       "      <td>59.592243</td>\n",
       "      <td>68.636468</td>\n",
       "      <td>73.158581</td>\n",
       "      <td>75.419637</td>\n",
       "      <td>79.941749</td>\n",
       "      <td>84.463861</td>\n",
       "      <td>88.985974</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Duration  TotalPackets  IncomingPackets  OutgoingPackets  InOutRatio  \\\n",
       "0  9.051183           221              175               46    3.804348   \n",
       "1  5.994780           174              136               38    3.578947   \n",
       "2  7.128145           250              202               48    4.208333   \n",
       "3  8.010340           208              166               42    3.952381   \n",
       "4  9.010757           214              175               39    4.487179   \n",
       "\n",
       "   TotalSize  IncomingSize  OutgoingSize  SizeRatio  OutgoingOrderingMean  \\\n",
       "0     378195        348485         29710  11.729552            239.217391   \n",
       "1     349012        324536         24476  13.259356            191.631579   \n",
       "2     425902        396040         29862  13.262340            257.333333   \n",
       "3     381302        354466         26836  13.208600            225.357143   \n",
       "4     357960        331866         26094  12.718096            238.717949   \n",
       "\n",
       "   ...  OutStream11  OutStream12  OutStream13  OutStream14  OutStream15  \\\n",
       "0  ...    58.296870    66.240323    68.226186    70.212050    72.197913   \n",
       "1  ...    61.431606    63.842131    75.894754    75.894754    78.305279   \n",
       "2  ...    52.581877    56.533387    64.436407    70.363673    72.339428   \n",
       "3  ...    53.830675    60.426293    69.220450    71.418989    75.816068   \n",
       "4  ...    50.548019    57.331187    59.592243    68.636468    73.158581   \n",
       "\n",
       "   OutStream16  OutStream17  OutStream18  OutStream19  OutStream20  \n",
       "0    74.183777    80.141367    92.056547    94.042410        100.0  \n",
       "1    80.715803    83.126328    85.536852    87.947377        100.0  \n",
       "2    76.290938    84.193959    86.169714    94.072735        100.0  \n",
       "3    78.014607    80.213147    82.411686    91.205843        100.0  \n",
       "4    75.419637    79.941749    84.463861    88.985974        100.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "        68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "        85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([20, 20, 20, 20, 20, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 20, 20, 20]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics, preprocessing, svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Duration', 'TotalPackets', 'IncomingPackets', 'OutgoingPackets', 'InOutRatio', 'TotalSize', 'IncomingSize', 'OutgoingSize', 'SizeRatio', 'OutgoingOrderingMean']\n",
      "['InStream1', 'InStream2', 'InStream3', 'InStream4', 'InStream5', 'InStream6', 'InStream7', 'InStream8', 'InStream9', 'InStream10', 'InStream11', 'InStream12', 'InStream13', 'InStream14', 'InStream15', 'InStream16', 'InStream17', 'InStream18', 'InStream19', 'InStream20', 'OutStream1', 'OutStream2', 'OutStream3', 'OutStream4', 'OutStream5', 'OutStream6', 'OutStream7', 'OutStream8', 'OutStream9', 'OutStream10', 'OutStream11', 'OutStream12', 'OutStream13', 'OutStream14', 'OutStream15', 'OutStream16', 'OutStream17', 'OutStream18', 'OutStream19', 'OutStream20']\n"
     ]
    }
   ],
   "source": [
    "all_columns = X.columns.tolist()\n",
    "columns = all_columns[:10]\n",
    "stream_ratios = all_columns[-40:]\n",
    "print(columns)\n",
    "print(stream_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_raw)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train_raw), columns=all_columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_raw), columns=all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== 1-NN ========\n",
      "Classification using 1 features\n",
      "Classification using 2 features\n",
      "Classification using 3 features\n",
      "Classification using 4 features\n",
      "Classification using 5 features\n",
      "Classification using 6 features\n",
      "Classification using 7 features\n",
      "Classification using 8 features\n",
      "Classification using 9 features\n",
      "Classification using 10 features\n",
      "\n",
      "\n",
      "Current max: 0.945 \n",
      "\n",
      "\n",
      "\n",
      "======== 2-NN ========\n",
      "Classification using 1 features\n",
      "Classification using 2 features\n",
      "Classification using 3 features\n",
      "Classification using 4 features\n",
      "Classification using 5 features\n",
      "Classification using 6 features\n",
      "Classification using 7 features\n",
      "Classification using 8 features\n",
      "Classification using 9 features\n",
      "Classification using 10 features\n",
      "\n",
      "\n",
      "Current max: 0.945 \n",
      "\n",
      "\n",
      "\n",
      "======== 3-NN ========\n",
      "Classification using 1 features\n",
      "Classification using 2 features\n",
      "Classification using 3 features\n",
      "Classification using 4 features\n",
      "Classification using 5 features\n",
      "Classification using 6 features\n",
      "Classification using 7 features\n",
      "Classification using 8 features\n",
      "Classification using 9 features\n",
      "Classification using 10 features\n",
      "\n",
      "\n",
      "Current max: 0.945 \n",
      "\n",
      "\n",
      "\n",
      "======== 4-NN ========\n",
      "Classification using 1 features\n",
      "Classification using 2 features\n",
      "Classification using 3 features\n",
      "Classification using 4 features\n",
      "Classification using 5 features\n",
      "Classification using 6 features\n",
      "Classification using 7 features\n",
      "Classification using 8 features\n",
      "Classification using 9 features\n",
      "Classification using 10 features\n",
      "\n",
      "\n",
      "Current max: 0.945 \n",
      "\n",
      "\n",
      "\n",
      "======== 5-NN ========\n",
      "Classification using 1 features\n",
      "Classification using 2 features\n",
      "Classification using 3 features\n",
      "Classification using 4 features\n",
      "Classification using 5 features\n",
      "Classification using 6 features\n",
      "Classification using 7 features\n",
      "Classification using 8 features\n",
      "Classification using 9 features\n",
      "Classification using 10 features\n",
      "\n",
      "\n",
      "Current max: 0.945 \n",
      "\n",
      "\n",
      "\n",
      "======== 6-NN ========\n",
      "Classification using 1 features\n",
      "Classification using 2 features\n",
      "Classification using 3 features\n",
      "Classification using 4 features\n",
      "Classification using 5 features\n",
      "Classification using 6 features\n",
      "Classification using 7 features\n",
      "Classification using 8 features\n",
      "Classification using 9 features\n",
      "Classification using 10 features\n",
      "\n",
      "\n",
      "Current max: 0.945 \n",
      "\n",
      "\n",
      "\n",
      "======== 7-NN ========\n",
      "Classification using 1 features\n",
      "Classification using 2 features\n",
      "Classification using 3 features\n",
      "Classification using 4 features\n",
      "Classification using 5 features\n",
      "Classification using 6 features\n",
      "Classification using 7 features\n",
      "Classification using 8 features\n",
      "Classification using 9 features\n",
      "Classification using 10 features\n",
      "\n",
      "\n",
      "Current max: 0.945 \n",
      "\n",
      "\n",
      "\n",
      "======== 8-NN ========\n",
      "Classification using 1 features\n",
      "Classification using 2 features\n",
      "Classification using 3 features\n",
      "Classification using 4 features\n",
      "Classification using 5 features\n",
      "Classification using 6 features\n",
      "Classification using 7 features\n",
      "Classification using 8 features\n",
      "Classification using 9 features\n",
      "Classification using 10 features\n",
      "\n",
      "\n",
      "Current max: 0.945 \n",
      "\n",
      "\n",
      "\n",
      "======== 9-NN ========\n",
      "Classification using 1 features\n",
      "Classification using 2 features\n",
      "Classification using 3 features\n",
      "Classification using 4 features\n",
      "Classification using 5 features\n",
      "Classification using 6 features\n",
      "Classification using 7 features\n",
      "Classification using 8 features\n",
      "Classification using 9 features\n",
      "Classification using 10 features\n",
      "\n",
      "\n",
      "Current max: 0.945 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_scores = []\n",
    "\n",
    "for k in range(1, 10):\n",
    "    print(f\"======== {k}-NN ========\")\n",
    "    \n",
    "    #split dataset into train and test data\n",
    "    features = stream_ratios\n",
    "    X_train = X_train_scaled[features]\n",
    "    X_test = X_test_scaled[features]\n",
    "    # Create KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Fit the classifier to the data\n",
    "    knn.fit(X_train,y_train)\n",
    "    #check accuracy of our model on the test data\n",
    "    feature_score = knn.score(X_test, y_test)\n",
    "    feature_scores.append((feature_score, 0, True, []))\n",
    "    \n",
    "    for n_features in range(1, len(columns)+1):\n",
    "        print(f\"Classification using {n_features} features\")\n",
    "        for feature_t in combinations(columns, n_features):\n",
    "            #split dataset into train and test data\n",
    "            features = list(feature_t)\n",
    "            X_train = X_train_scaled[features]\n",
    "            X_test = X_test_scaled[features]\n",
    "            # Create KNN classifier\n",
    "            knn1 = KNeighborsClassifier(n_neighbors=k)\n",
    "            # Fit the classifier to the data\n",
    "            knn1.fit(X_train,y_train)\n",
    "            #check accuracy of our model on the test data\n",
    "            feature_score = knn1.score(X_test, y_test)\n",
    "            feature_scores.append((feature_score, k, False, features))\n",
    "            \n",
    "            #split dataset into train and test data\n",
    "            features_extended = features + stream_ratios\n",
    "            X_train = X_train_scaled[features_extended]\n",
    "            X_test = X_test_scaled[features_extended]\n",
    "            # Create KNN classifier\n",
    "            knn2 = KNeighborsClassifier(n_neighbors=k)\n",
    "            # Fit the classifier to the data\n",
    "            knn2.fit(X_train,y_train)\n",
    "            #check accuracy of our model on the test data\n",
    "            feature_score = knn2.score(X_test, y_test)\n",
    "            feature_scores.append((feature_score, k, True, features))\n",
    "    print('\\n')   \n",
    "    print(\"Current max:\", max(feature_scores)[0], '\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.945, 1, True, ['TotalPackets', 'OutgoingPackets', 'TotalSize', 'IncomingSize', 'OutgoingSize', 'SizeRatio', 'OutgoingOrderingMean'])\n"
     ]
    }
   ],
   "source": [
    "print(max(feature_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_extended = ['TotalPackets', 'OutgoingPackets', 'TotalSize', 'IncomingSize', 'OutgoingSize', 'SizeRatio', 'OutgoingOrderingMean'] + stream_ratios\n",
    "X_selected = X[features_extended]\n",
    "scaler = preprocessing.StandardScaler().fit(X_selected)\n",
    "X_scaled = pd.DataFrame(scaler.transform(X_selected), columns=features_extended)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check accuracy of our model on the test data\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67         4\n",
      "           1       0.80      1.00      0.89         4\n",
      "           2       0.67      0.50      0.57         4\n",
      "           3       0.67      0.50      0.57         4\n",
      "           4       0.67      1.00      0.80         4\n",
      "           5       0.75      0.75      0.75         4\n",
      "           6       0.40      0.50      0.44         4\n",
      "           7       0.50      0.50      0.50         4\n",
      "           8       0.50      1.00      0.67         4\n",
      "           9       0.67      1.00      0.80         4\n",
      "          10       0.50      0.50      0.50         4\n",
      "          11       0.67      1.00      0.80         4\n",
      "          12       0.50      0.75      0.60         4\n",
      "          13       1.00      0.75      0.86         4\n",
      "          14       1.00      0.75      0.86         4\n",
      "          15       1.00      0.75      0.86         4\n",
      "          16       1.00      1.00      1.00         4\n",
      "          17       0.80      1.00      0.89         4\n",
      "          18       0.67      0.50      0.57         4\n",
      "          19       0.57      1.00      0.73         4\n",
      "          20       0.60      0.75      0.67         4\n",
      "          21       0.75      0.75      0.75         4\n",
      "          22       1.00      0.75      0.86         4\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       0.67      1.00      0.80         4\n",
      "          25       0.80      1.00      0.89         4\n",
      "          26       1.00      0.75      0.86         4\n",
      "          27       0.40      0.50      0.44         4\n",
      "          28       0.50      0.50      0.50         4\n",
      "          29       0.50      0.75      0.60         4\n",
      "          30       0.60      0.75      0.67         4\n",
      "          31       1.00      0.50      0.67         4\n",
      "          32       0.60      0.75      0.67         4\n",
      "          33       0.75      0.75      0.75         4\n",
      "          34       0.60      0.75      0.67         4\n",
      "          35       1.00      1.00      1.00         4\n",
      "          36       0.67      0.50      0.57         4\n",
      "          37       1.00      0.75      0.86         4\n",
      "          38       1.00      0.50      0.67         4\n",
      "          39       0.75      0.75      0.75         4\n",
      "          40       1.00      1.00      1.00         4\n",
      "          41       0.33      0.25      0.29         4\n",
      "          42       0.60      0.75      0.67         4\n",
      "          43       1.00      0.75      0.86         4\n",
      "          44       0.80      1.00      0.89         4\n",
      "          45       0.80      1.00      0.89         4\n",
      "          46       0.60      0.75      0.67         4\n",
      "          47       0.60      0.75      0.67         4\n",
      "          48       0.50      0.50      0.50         4\n",
      "          49       1.00      0.25      0.40         4\n",
      "          50       0.80      1.00      0.89         4\n",
      "          51       1.00      0.25      0.40         4\n",
      "          52       1.00      0.75      0.86         4\n",
      "          53       1.00      0.50      0.67         4\n",
      "          54       1.00      0.50      0.67         4\n",
      "          55       1.00      0.75      0.86         4\n",
      "          56       0.60      0.75      0.67         4\n",
      "          57       0.50      0.50      0.50         4\n",
      "          58       0.67      0.50      0.57         4\n",
      "          59       0.50      0.50      0.50         4\n",
      "          60       0.50      0.50      0.50         4\n",
      "          61       0.67      0.50      0.57         4\n",
      "          62       0.50      0.25      0.33         4\n",
      "          63       0.80      1.00      0.89         4\n",
      "          64       0.60      0.75      0.67         4\n",
      "          65       1.00      0.25      0.40         4\n",
      "          66       0.80      1.00      0.89         4\n",
      "          67       1.00      0.50      0.67         4\n",
      "          68       1.00      0.75      0.86         4\n",
      "          69       0.75      0.75      0.75         4\n",
      "          70       0.50      1.00      0.67         4\n",
      "          71       0.75      0.75      0.75         4\n",
      "          72       0.75      0.75      0.75         4\n",
      "          73       0.80      1.00      0.89         4\n",
      "          74       0.75      0.75      0.75         4\n",
      "          75       0.33      0.25      0.29         4\n",
      "          76       0.67      0.50      0.57         4\n",
      "          77       1.00      0.50      0.67         4\n",
      "          78       1.00      0.50      0.67         4\n",
      "          79       1.00      0.75      0.86         4\n",
      "          80       1.00      0.50      0.67         4\n",
      "          81       0.80      1.00      0.89         4\n",
      "          82       0.67      0.50      0.57         4\n",
      "          83       1.00      0.75      0.86         4\n",
      "          84       0.67      1.00      0.80         4\n",
      "          85       0.60      0.75      0.67         4\n",
      "          86       0.50      0.50      0.50         4\n",
      "          87       1.00      0.75      0.86         4\n",
      "          88       1.00      0.75      0.86         4\n",
      "          89       1.00      0.75      0.86         4\n",
      "          90       1.00      1.00      1.00         4\n",
      "          91       0.75      0.75      0.75         4\n",
      "          92       0.60      0.75      0.67         4\n",
      "          93       0.67      1.00      0.80         4\n",
      "          94       1.00      1.00      1.00         4\n",
      "          95       1.00      1.00      1.00         4\n",
      "          96       1.00      1.00      1.00         4\n",
      "          97       0.67      0.50      0.57         4\n",
      "          98       0.40      0.50      0.44         4\n",
      "          99       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.72       400\n",
      "   macro avg       0.76      0.72      0.71       400\n",
      "weighted avg       0.76      0.72      0.71       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.645      0.71       0.775      0.755      0.685      0.805\n",
      " 0.69       0.60301508 0.74371859 0.7638191 ]\n",
      "cv_scores mean:0.7175552763819095\n"
     ]
    }
   ],
   "source": [
    "clf_cv = svm.SVC(kernel='linear')\n",
    "\n",
    "#train model with cv of 10\n",
    "cv_scores = cross_val_score(clf_cv, X_scaled, y, cv=10)\n",
    "\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
